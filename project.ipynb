{
  "cells": [
    {
      "metadata": {
        "id": "ccd6c8e71c0953a7"
      },
      "cell_type": "markdown",
      "source": [
        "### 파이썬 버전 확인"
      ],
      "id": "ccd6c8e71c0953a7"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zq011bQGe8ZH",
        "outputId": "efa484d7-8da1-4e1e-e7fd-5f42a4d53fef",
        "ExecuteTime": {
          "end_time": "2025-12-31T05:48:50.316563Z",
          "start_time": "2025-12-31T05:48:50.248568Z"
        }
      },
      "id": "zq011bQGe8ZH",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.11.14 (main, Oct 21 2025, 18:27:30) [Clang 20.1.8 ]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fa64466eca1790b6"
      },
      "cell_type": "markdown",
      "source": [
        "## 네이버 쇼핑 리뷰를 보고 판매자 답변 자동 생성하기(정중/사과/보상 제안)"
      ],
      "id": "fa64466eca1790b6"
    },
    {
      "metadata": {
        "id": "623f33a35a750969"
      },
      "cell_type": "markdown",
      "source": [
        "#### 데이터 출처 : https://github.com/bab2min/corpus"
      ],
      "id": "623f33a35a750969"
    },
    {
      "metadata": {
        "id": "49d892c43b5ef03f"
      },
      "cell_type": "markdown",
      "source": [
        "1. 라이브러리 설치 (Colab GPU / Python 3.11)"
      ],
      "id": "49d892c43b5ef03f"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-31T05:48:56.603556Z",
          "start_time": "2025-12-31T05:48:53.821756Z"
        },
        "id": "8cddb851f8b3d441"
      },
      "cell_type": "code",
      "source": [
        "# Colab은 보통 torch 2.6.0+cu124가 이미 설치되어 있습니다.\n",
        "# 설치가 안 되어 있거나 버전을 맞추고 싶다면 주석 해제하여 실행하세요.\n",
        "# --- 맥북(Apple Silicon, CPU/MPS) ---\n",
        "#!pip install -q -U pip\n",
        "#!pip install -q \"torch>=2.2\" \"torchvision>=0.17\" \"torchaudio>=2.2\" \\\n",
        "#   \"transformers>=4.41\" \"datasets>=2.19\" accelerate evaluate \\\n",
        "#    \"scikit-learn>=1.2,<1.7\" tqdm peft\n",
        "\n",
        "# --- Colab GPU(T4 등, CUDA 12.4) ---\n",
        "# Colab에 기본 설치된 torch 2.6.0+cu124가 있으면 이 블록은 생략해도 됩니다.\n",
        "# 없거나 버전을 맞추고 싶을 때만 주석 해제해서 실행하세요.\n",
        "# !pip install -q --index-url https://download.pytorch.org/whl/cu124 \\\n",
        "#     torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124\n",
        "# 공통 패키지 설치\n",
        "!pip install -q -U \"transformers>=4.41\" \"datasets>=2.19\" accelerate evaluate \\\n",
        "    \"scikit-learn>=1.2,<1.7\" tqdm peft"
      ],
      "id": "8cddb851f8b3d441",
      "outputs": [],
      "execution_count": 15
    },
    {
      "metadata": {
        "id": "74d0ec5f2144ba54"
      },
      "cell_type": "markdown",
      "source": [
        "2. 데이터 파싱"
      ],
      "id": "74d0ec5f2144ba54"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd4f60c06066e59",
        "outputId": "d1cd6bad-dde7-4e81-fe09-b686f6531324",
        "ExecuteTime": {
          "end_time": "2025-12-31T05:49:03.970774Z",
          "start_time": "2025-12-31T05:49:03.752921Z"
        }
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "RAW_PATH = Path(\"./naver_shopping.txt\")\n",
        "\n",
        "def load_reviews(min_len=5, max_n=None, seed=42):\n",
        "    random.seed(seed)\n",
        "    rows = []\n",
        "    with RAW_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(\"\\t\", 1)\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "            rating_s, text = parts\n",
        "            try:\n",
        "                rating = int(rating_s)\n",
        "            except:\n",
        "                continue\n",
        "            text = text.strip()\n",
        "            if len(text) < min_len:\n",
        "                continue\n",
        "            rows.append((rating, text))\n",
        "    random.shuffle(rows)\n",
        "    if max_n:\n",
        "        rows = rows[:max_n]\n",
        "    return rows\n",
        "\n",
        "rows = load_reviews(max_n=30000)  # 8GB면 처음엔 3만 이하 추천\n",
        "print(\"loaded:\", len(rows))\n",
        "print(rows[0])"
      ],
      "id": "dd4f60c06066e59",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded: 30000\n",
            "(2, '냄새랑 맛이 생각보다 너무 역해요')\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "metadata": {
        "id": "be7f27b09dd7efc"
      },
      "cell_type": "markdown",
      "source": [
        "3. 불만 유형(키워드) 분류"
      ],
      "id": "be7f27b09dd7efc"
    },
    {
      "metadata": {
        "id": "dbc5da775caaccc2",
        "ExecuteTime": {
          "end_time": "2025-12-31T05:49:09.777103Z",
          "start_time": "2025-12-31T05:49:09.766866Z"
        }
      },
      "cell_type": "code",
      "source": [
        "COMPLAINT_RULES = {\n",
        "    \"배송\": [\"배송\", \"늦\", \"지연\", \"도착\", \"택배\"],\n",
        "    \"품질/불량\": [\"불량\", \"하자\", \"고장\", \"깨\", \"찢\", \"누수\", \"작동\", \"불안정\"],\n",
        "    \"포장\": [\"포장\", \"박스\", \"파손\", \"훼손\", \"완충\"],\n",
        "    \"가격/가성비\": [\"비싸\", \"가격\", \"가성비\", \"싸\", \"할인\"],\n",
        "    \"응대/서비스\": [\"응대\", \"문의\", \"연락\", \"고객센터\", \"불친절\"],\n",
        "}\n",
        "\n",
        "def detect_topics(text: str):\n",
        "    topics = []\n",
        "    for topic, kws in COMPLAINT_RULES.items():\n",
        "        if any(kw in text for kw in kws):\n",
        "            topics.append(topic)\n",
        "    return topics[:2]  # 너무 많으면 1~2개만"
      ],
      "id": "dbc5da775caaccc2",
      "outputs": [],
      "execution_count": 17
    },
    {
      "metadata": {
        "id": "b711a984fd0708ca"
      },
      "cell_type": "markdown",
      "source": [
        "4. 템플릿 기반 판매자 답변 생성"
      ],
      "id": "b711a984fd0708ca"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "801fe53949cf5aa7",
        "outputId": "a3f7a889-4fb5-4998-ef3f-d89cf3629109",
        "ExecuteTime": {
          "end_time": "2025-12-31T05:49:12.904240Z",
          "start_time": "2025-12-31T05:49:12.868637Z"
        }
      },
      "cell_type": "code",
      "source": [
        "def make_seller_reply(rating: int, review: str) -> str:\n",
        "    topics = detect_topics(review)\n",
        "    topic_phrase = \" / \".join(topics) if topics else None\n",
        "\n",
        "    # 보상 제안(너무 구체적 금액은 피하고 옵션만 제시)\n",
        "    compensation = random.choice([\n",
        "        \"교환 또는 환불을 도와드리겠습니다.\",\n",
        "        \"확인 후 쿠폰/부분환불 등 가능한 보상안을 안내드리겠습니다.\",\n",
        "        \"불편을 줄이기 위해 교환/환불 절차를 빠르게 진행해드리겠습니다.\"\n",
        "    ])\n",
        "\n",
        "    # 긍정(4~5)\n",
        "    if rating >= 4:\n",
        "        extra = \"앞으로도 더 좋은 상품과 서비스로 보답하겠습니다.\"\n",
        "        upsell = random.choice([\n",
        "            \"재구매해주시면 감사하겠습니다.\",\n",
        "            \"다음에도 만족스러운 경험을 드리겠습니다.\",\n",
        "            \"소중한 후기 감사합니다.\"\n",
        "        ])\n",
        "        if topic_phrase:\n",
        "            return f\"안녕하세요, 고객님. {topic_phrase} 관련하여 만족하셨다니 정말 기쁩니다. {extra} {upsell}\"\n",
        "        return f\"안녕하세요, 고객님. 소중한 후기 감사합니다. {extra} {upsell}\"\n",
        "\n",
        "    # 부정(1~2)\n",
        "    if rating <= 2:\n",
        "        apology = \"불편을 드려 진심으로 죄송합니다.\"\n",
        "        ask = \"주문 정보와 문제 상황을 확인할 수 있도록 문의 남겨주시면 빠르게 도와드리겠습니다.\"\n",
        "        if topic_phrase:\n",
        "            return f\"안녕하세요, 고객님. {topic_phrase} 관련하여 {apology} {ask} {compensation}\"\n",
        "        return f\"안녕하세요, 고객님. {apology} {ask} {compensation}\"\n",
        "\n",
        "    # 중립(3) — 과제에서는 제외해도 되고, 남겨도 됨\n",
        "    neutral = \"의견 남겨주셔서 감사합니다.\"\n",
        "    improve = \"말씀해주신 부분은 개선하여 더 나은 서비스로 보답하겠습니다.\"\n",
        "    if topic_phrase:\n",
        "        return f\"안녕하세요, 고객님. {topic_phrase} 관련하여 {neutral} {improve}\"\n",
        "    return f\"안녕하세요, 고객님. {neutral} {improve}\"\n",
        "\n",
        "# 샘플 확인\n",
        "for r, t in rows[:5]:\n",
        "    print(\"RATING:\", r)\n",
        "    print(\"REVIEW:\", t)\n",
        "    print(\"REPLY:\", make_seller_reply(r, t))\n",
        "    print(\"-\"*80)"
      ],
      "id": "801fe53949cf5aa7",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RATING: 2\n",
            "REVIEW: 냄새랑 맛이 생각보다 너무 역해요\n",
            "REPLY: 안녕하세요, 고객님. 불편을 드려 진심으로 죄송합니다. 주문 정보와 문제 상황을 확인할 수 있도록 문의 남겨주시면 빠르게 도와드리겠습니다. 확인 후 쿠폰/부분환불 등 가능한 보상안을 안내드리겠습니다.\n",
            "--------------------------------------------------------------------------------\n",
            "RATING: 5\n",
            "REVIEW: 진짜대박좋아요만족^^\n",
            "REPLY: 안녕하세요, 고객님. 소중한 후기 감사합니다. 앞으로도 더 좋은 상품과 서비스로 보답하겠습니다. 다음에도 만족스러운 경험을 드리겠습니다.\n",
            "--------------------------------------------------------------------------------\n",
            "RATING: 5\n",
            "REVIEW: 좋아요 배송도 빠르고 가겨도 마니 저렴하고~~^^\n",
            "REPLY: 안녕하세요, 고객님. 배송 관련하여 만족하셨다니 정말 기쁩니다. 앞으로도 더 좋은 상품과 서비스로 보답하겠습니다. 다음에도 만족스러운 경험을 드리겠습니다.\n",
            "--------------------------------------------------------------------------------\n",
            "RATING: 1\n",
            "REVIEW: 1년 넘게 쓴 와이퍼보다 능력이 떨어지내요. 워셔액도 못 닦아내네요. 이런거 팔지 않으셨으면 좋겠네요\n",
            "REPLY: 안녕하세요, 고객님. 불편을 드려 진심으로 죄송합니다. 주문 정보와 문제 상황을 확인할 수 있도록 문의 남겨주시면 빠르게 도와드리겠습니다. 확인 후 쿠폰/부분환불 등 가능한 보상안을 안내드리겠습니다.\n",
            "--------------------------------------------------------------------------------\n",
            "RATING: 2\n",
            "REVIEW: 진짜뚱뚱하신분만사셔야될듯\n",
            "REPLY: 안녕하세요, 고객님. 불편을 드려 진심으로 죄송합니다. 주문 정보와 문제 상황을 확인할 수 있도록 문의 남겨주시면 빠르게 도와드리겠습니다. 교환 또는 환불을 도와드리겠습니다.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "metadata": {
        "id": "519a046a30f10c57"
      },
      "cell_type": "markdown",
      "source": [
        "5. 학습 데이터 구성 + split"
      ],
      "id": "519a046a30f10c57"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5e5a366160b74a4",
        "outputId": "4fd751bb-1588-4185-f5e7-de5e369a2ad4",
        "ExecuteTime": {
          "end_time": "2025-12-31T05:49:17.584362Z",
          "start_time": "2025-12-31T05:49:17.406684Z"
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def build_example(rating, review):\n",
        "    # 프롬프트: 역할/규칙/입력\n",
        "    prompt = (\n",
        "        \"### 역할: 당신은 온라인 쇼핑몰 판매자입니다.\\n\"\n",
        "        \"### 규칙:\\n\"\n",
        "        \"- 한국어 존댓말로 정중하게 작성\\n\"\n",
        "        \"- 2~4문장으로 간결하게\\n\"\n",
        "        \"- 부정 리뷰(1~2점)에는 사과+해결책+보상(쿠폰/교환/환불 중 하나) 포함\\n\"\n",
        "        \"- 긍정 리뷰(4~5점)에는 감사+재구매/재방문 유도 포함\\n\"\n",
        "        \"### 고객 리뷰:\\n\"\n",
        "        f\"{review}\\n\"\n",
        "        \"### 판매자 답변:\\n\"\n",
        "    )\n",
        "    completion = make_seller_reply(rating, review)\n",
        "    return {\"prompt\": prompt, \"completion\": completion, \"rating\": rating, \"review\": review}\n",
        "\n",
        "# 3점은 빼서 “명확한” 긍/부정만 학습(추천)\n",
        "filtered = [(r, t) for (r, t) in rows if r in (1,2,4,5)]\n",
        "data = [build_example(r, t) for r, t in filtered[:20000]]  # 최대 2만개만\n",
        "\n",
        "train_data, valid_data = train_test_split(data, test_size=0.05, random_state=42)\n",
        "\n",
        "print(len(train_data), len(valid_data))\n",
        "print(train_data[0][\"prompt\"])\n",
        "print(\"->\", train_data[0][\"completion\"])"
      ],
      "id": "a5e5a366160b74a4",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19000 1000\n",
            "### 역할: 당신은 온라인 쇼핑몰 판매자입니다.\n",
            "### 규칙:\n",
            "- 한국어 존댓말로 정중하게 작성\n",
            "- 2~4문장으로 간결하게\n",
            "- 부정 리뷰(1~2점)에는 사과+해결책+보상(쿠폰/교환/환불 중 하나) 포함\n",
            "- 긍정 리뷰(4~5점)에는 감사+재구매/재방문 유도 포함\n",
            "### 고객 리뷰:\n",
            "저렴하게 좋은 제품을 구매한거 같아 만족해요^^\n",
            "### 판매자 답변:\n",
            "\n",
            "-> 안녕하세요, 고객님. 소중한 후기 감사합니다. 앞으로도 더 좋은 상품과 서비스로 보답하겠습니다. 재구매해주시면 감사하겠습니다.\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "metadata": {
        "id": "63d01d32d340f820"
      },
      "cell_type": "markdown",
      "source": [
        "6. 모델/토크나이저 로드 + LoRA 적용"
      ],
      "id": "63d01d32d340f820"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37a82ec5c10af318",
        "outputId": "b3492aab-957e-4404-e17c-5fde805523db",
        "ExecuteTime": {
          "end_time": "2025-12-31T05:49:38.804849Z",
          "start_time": "2025-12-31T05:49:30.366766Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import torch, platform\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "MODEL_ID = \"skt/kogpt2-base-v2\"\n",
        "def pick_device():\n",
        "    # Colab GPU (CUDA)\n",
        "    if torch.cuda.is_available():\n",
        "        return \"cuda\"\n",
        "    # M1/M2 Apple Silicon (MPS)\n",
        "    if platform.system() == \"Darwin\" and torch.backends.mps.is_available():\n",
        "        return \"mps\"\n",
        "    # fallback CPU\n",
        "    return \"cpu\"\n",
        "device = pick_device()\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "print(\"mps available:\", torch.backends.mps.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"gpu name:\", torch.cuda.get_device_name(0))\n",
        "print(\"device:\", device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "\n",
        "# GPT2류는 pad_token이 없는 경우가 많아서 eos로 맞추는 게 안전\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
        "model.config.use_cache = False\n",
        "model.gradient_checkpointing_enable()  # 메모리 절약\n",
        "\n",
        "# LoRA 설정 (GPT-2 계열에서 흔히 쓰는 target)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 계열에 흔한 모듈명\n",
        "    fan_in_fan_out=True\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "model.to(device)"
      ],
      "id": "37a82ec5c10af318",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.6.0+cu124\n",
            "cuda available: True\n",
            "mps available: False\n",
            "gpu name: Tesla T4\n",
            "device: cuda\n",
            "trainable params: 811,008 || all params: 125,975,040 || trainable%: 0.6438\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): GPT2LMHeadModel(\n",
              "      (transformer): GPT2Model(\n",
              "        (wte): Embedding(51200, 768)\n",
              "        (wpe): Embedding(1024, 768)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (h): ModuleList(\n",
              "          (0-11): 12 x GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2Attention(\n",
              "              (c_attn): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=2304, nx=768)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (c_proj): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=768, nx=768)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D(nf=3072, nx=768)\n",
              "              (c_proj): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=768, nx=3072)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "execution_count": 20
    },
    {
      "metadata": {
        "id": "8556d011fc204819"
      },
      "cell_type": "markdown",
      "source": [
        "7. Dataset 만들기 + 토크나이징"
      ],
      "id": "8556d011fc204819"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "9ee281eaebcf4167a8ef8b020ad57c9f",
            "7eabb2708d6b43ab80fad23a882a155d",
            "ad549afa0da14eff9b17a4b12fd57078",
            "7f8cdfb62e974c77899f0b2f33478d57",
            "d4247916794b49648448dbcea3892d3c",
            "535348c4647543f4a9c26f156ca389a1",
            "e17deef0ee544b7fbb6818071c3f8212",
            "a9ab9201543942ec9b4700908f1d9b54",
            "03abd0257a7749f6aa57d26476528e93",
            "d99101f26b114e38a087566f4a6868c9",
            "461fab2872ca427dad527ee966526a7f",
            "db9590428ff94c27af62b512f23d94ec",
            "2be5fb1e06254dc7b2719a97cc5ab560",
            "12ebce4469b84279ba96867125329f3f",
            "99355e111b0049dc818ff880ba14276e",
            "ffb57ae8f98c4da090b9a8d904ac1188",
            "d6e8e8522761447b8cb7829e47e0e375",
            "ce88bb889f66474d8a16dc8c13c582bf",
            "a5e255c6683e4865a4c37672db7f2d37",
            "81ab9386dd05409d9f9459e8d015b428",
            "a88ccef373034f3688298cdf8c56b03a",
            "d4232ca1a1494ce096f252ebe6f53212"
          ]
        },
        "id": "7ef416af57b56da8",
        "outputId": "428fce37-d6b8-417e-8d17-7479aa9b8fea",
        "ExecuteTime": {
          "end_time": "2025-12-31T05:49:43.899773Z",
          "start_time": "2025-12-31T05:49:41.763077Z"
        }
      },
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def pack_text(ex):\n",
        "    return ex[\"prompt\"] + ex[\"completion\"]\n",
        "\n",
        "train_ds = Dataset.from_dict({\"text\": [pack_text(x) for x in train_data]})\n",
        "valid_ds = Dataset.from_dict({\"text\": [pack_text(x) for x in valid_data]})\n",
        "\n",
        "MAX_LEN = 192  # 8GB 추천(256도 가능하지만 터질 수 있음)\n",
        "\n",
        "def tok(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "train_tok = train_ds.map(tok, batched=True, remove_columns=[\"text\"])\n",
        "valid_tok = valid_ds.map(tok, batched=True, remove_columns=[\"text\"])\n"
      ],
      "id": "7ef416af57b56da8",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/19000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ee281eaebcf4167a8ef8b020ad57c9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db9590428ff94c27af62b512f23d94ec"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 21
    },
    {
      "metadata": {
        "id": "40afa70a49585bb9"
      },
      "cell_type": "markdown",
      "source": [
        "8. Trainer로 학습"
      ],
      "id": "40afa70a49585bb9"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "60bf0f086be1ca5a",
        "outputId": "246f506e-d64f-46ab-c79a-1c7785ebd63f",
        "ExecuteTime": {
          "end_time": "2025-12-31T05:50:21.622761Z",
          "start_time": "2025-12-31T05:49:48.545779Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import os, random, inspect, platform\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# 디바이스 선택 (CUDA 우선, 그다음 MPS, 아니면 CPU)\n",
        "def pick_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return \"cuda\"\n",
        "    if platform.system() == \"Darwin\" and torch.backends.mps.is_available():\n",
        "        return \"mps\"\n",
        "    return \"cpu\"\n",
        "\n",
        "device = pick_device()\n",
        "print(\"torch =\", torch.__version__)\n",
        "print(\"cuda available =\", torch.cuda.is_available())\n",
        "print(\"mps available =\", torch.backends.mps.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"gpu name =\", torch.cuda.get_device_name(0))\n",
        "print(\"device =\", device)\n",
        "\n",
        "# 전제 체크\n",
        "assert \"tokenizer\" in globals(), \"tokenizer가 먼저 정의돼야 합니다.\"\n",
        "assert \"train_tok\" in globals(), \"train_tok(토큰화된 학습 데이터셋)이 먼저 정의돼야 합니다.\"\n",
        "valid_tok = globals().get(\"valid_tok\", None)\n",
        "MODEL_ID = globals().get(\"MODEL_ID\", \"skt/kogpt2-base-v2\")\n",
        "\n",
        "# tokenizer / special token 안전 확인\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"len(tokenizer) =\", len(tokenizer))\n",
        "print(\"pad_token_id =\", tokenizer.pad_token_id, \"eos_token_id =\", tokenizer.eos_token_id)\n",
        "\n",
        "# 베이스 모델 로드 + 임베딩 리사이즈\n",
        "base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "if tokenizer.eos_token_id is not None:\n",
        "    base_model.config.eos_token_id = tokenizer.eos_token_id\n",
        "base_model.config.use_cache = False\n",
        "base_model.gradient_checkpointing_enable()\n",
        "\n",
        "# LoRA 설정\n",
        "lora_config = globals().get(\"lora_config\", None)\n",
        "if lora_config is None:\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\"c_attn\"],\n",
        "    )\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "# 디바이스로 이동 (CUDA/MPS/CPU)\n",
        "model.to(device)\n",
        "try:\n",
        "    model.print_trainable_parameters()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 데이터 콜레이터\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# TrainingArguments\n",
        "OUTPUT_DIR = \"outputs/seller_reply_lora\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "common_kwargs = dict(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=2e-4,\n",
        "    max_steps=200,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=25,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    fp16=(device == \"cuda\"),  # CUDA에서만 fp16, MPS/CPU는 float32\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "try:\n",
        "    args = TrainingArguments(\n",
        "        **common_kwargs,\n",
        "        eval_strategy=\"steps\" if valid_tok is not None else \"no\",\n",
        "        eval_steps=200 if valid_tok is not None else None,\n",
        "    )\n",
        "except TypeError:\n",
        "    args = TrainingArguments(\n",
        "        **common_kwargs,\n",
        "        evaluation_strategy=\"steps\" if valid_tok is not None else \"no\",\n",
        "        eval_steps=200 if valid_tok is not None else None,\n",
        "    )\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=valid_tok if valid_tok is not None else None,\n",
        "    data_collator=collator,\n",
        ")\n",
        "if hasattr(trainer, \"label_names\"):\n",
        "    trainer.label_names = [\"labels\"]\n",
        "\n",
        "# 범위 체크\n",
        "emb_size = model.get_input_embeddings().weight.shape[0]\n",
        "idxs = random.sample(range(len(train_tok)), k=min(200, len(train_tok)))\n",
        "mx = max(max(train_tok[i][\"input_ids\"]) for i in idxs)\n",
        "print(\"model emb size =\", emb_size, \"| sample input_id max =\", mx)\n",
        "assert mx < emb_size, f\"토큰 id({mx})가 임베딩 크기({emb_size}) 이상입니다. resize가 적용됐는지 확인하세요.\"\n",
        "\n",
        "# 학습\n",
        "train_result = trainer.train()\n",
        "\n",
        "# 저장\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"✅ Done. Saved to:\", OUTPUT_DIR)\n",
        "print(\"Train result:\", train_result)"
      ],
      "id": "60bf0f086be1ca5a",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch = 2.6.0+cu124\n",
            "cuda available = True\n",
            "mps available = False\n",
            "gpu name = Tesla T4\n",
            "device = cuda\n",
            "len(tokenizer) = 51201\n",
            "pad_token_id = 51200 eos_token_id = 51200\n",
            "trainable params: 811,008 || all params: 125,975,808 || trainable%: 0.6438\n",
            "model emb size = 51201 | sample input_id max = 51200\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 05:33, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.691600</td>\n",
              "      <td>0.653286</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done. Saved to: outputs/seller_reply_lora\n",
            "Train result: TrainOutput(global_step=200, training_loss=1.230308208465576, metrics={'train_runtime': 335.354, 'train_samples_per_second': 9.542, 'train_steps_per_second': 0.596, 'total_flos': 316540138291200.0, 'train_loss': 1.230308208465576, 'epoch': 0.16842105263157894})\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "metadata": {
        "id": "d2d37e368bf3342d"
      },
      "cell_type": "markdown",
      "source": [
        "9. 생성 함수"
      ],
      "id": "d2d37e368bf3342d"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ff2c89fbd72024e",
        "outputId": "b876c34e-2ee3-4904-f69a-3b9b1c1e7af1"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device = cuda\n",
            "tokenizer class = GPT2TokenizerFast\n",
            "len(tokenizer) = 51201\n",
            "bos: 51200 eos: 51200 pad: 3\n",
            "emb size       = 51201\n",
            "vocab==emb ?   = True\n",
            "================================================================================\n",
            "Tokenizer sanity: has None in input_ids? -> False\n",
            "Tokenizer decode sanity: OK\n",
            "================================================================================\n",
            "Quick generation test:\n",
            "안녕하세요, 고객님. 배송 관련하여 불편을 드려 진심으로 죄송합니다. 주문 정보와 문제 상황을 확인할 수 있도록 문의 남겨주시면 빠르게 도와드리겠습니다. 교환/환불 절차를 빠르게 진행하겠습니다. 확인 후 쿠폰/교환 중 하나를 택배로 보상해 드리겠습니다.\n"
          ]
        }
      ],
      "execution_count": 48,
      "source": [
        "import torch, platform, re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "from typing import Union, List, Dict, Any\n",
        "\n",
        "# =========================\n",
        "# Config\n",
        "# =========================\n",
        "MODEL_ID = \"skt/kogpt2-base-v2\"\n",
        "CKPT_DIR = \"outputs/seller_reply_lora\"\n",
        "\n",
        "# =========================\n",
        "# Device picker\n",
        "# =========================\n",
        "def pick_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return \"cuda\"\n",
        "    if platform.system() == \"Darwin\" and torch.backends.mps.is_available():\n",
        "        return \"mps\"\n",
        "    return \"cpu\"\n",
        "\n",
        "device = pick_device()\n",
        "print(\"device =\", device)\n",
        "\n",
        "# =========================\n",
        "# 1) ✅ Tokenizer: 베이스 모델에서 로드 (fast 사용)\n",
        "# =========================\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "except Exception as e:\n",
        "    print(\"AutoTokenizer(use_fast=True) failed ->\", repr(e))\n",
        "    raise\n",
        "\n",
        "print(\"tokenizer class =\", tokenizer.__class__.__name__)\n",
        "\n",
        "# (안전) eos/pad 보장\n",
        "# GPT2 계열은 pad_token이 없는 경우가 많아 새 pad 토큰을 추가하는 방식이 가장 안전함\n",
        "if tokenizer.eos_token is None:\n",
        "    # KoGPT2는 보통 eos가 있지만 혹시 없으면 지정\n",
        "    tokenizer.eos_token = \"<|endoftext|>\"\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    # eos를 pad로 \"대입\"만 해도 되지만, 학습/패딩에서 더 안전하게 새 pad를 추가\n",
        "    # (원래 vocab에 없는 토큰이면 길이가 +1 됨)\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
        "\n",
        "print(\"len(tokenizer) =\", len(tokenizer))\n",
        "print(\"bos:\", tokenizer.bos_token_id, \"eos:\", tokenizer.eos_token_id, \"pad:\", tokenizer.pad_token_id)\n",
        "\n",
        "# =========================\n",
        "# 2) Base model load\n",
        "# =========================\n",
        "base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
        "\n",
        "# tokenizer 길이에 맞춰 임베딩 리사이즈 (pad 추가했으면 +1 됨)\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "base_model.config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# =========================\n",
        "# 3) Load LoRA adapter\n",
        "# =========================\n",
        "model = PeftModel.from_pretrained(base_model, CKPT_DIR)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"emb size       =\", model.get_input_embeddings().weight.shape[0])\n",
        "print(\"vocab==emb ?   =\", len(tokenizer) == model.get_input_embeddings().weight.shape[0])\n",
        "\n",
        "# ============================================================\n",
        "# Debugging block A: tokenizer sanity\n",
        "# ============================================================\n",
        "print(\"=\" * 80)\n",
        "test_text = \"리뷰: 배송이 늦어요\\n판매자 답변:\"\n",
        "enc = tokenizer(test_text, add_special_tokens=True)\n",
        "has_none = any(x is None for x in enc[\"input_ids\"])\n",
        "print(\"Tokenizer sanity: has None in input_ids? ->\", has_none)\n",
        "\n",
        "try:\n",
        "    _ = tokenizer.decode(enc[\"input_ids\"], skip_special_tokens=False)\n",
        "    print(\"Tokenizer decode sanity: OK\")\n",
        "except Exception as e:\n",
        "    print(\"Tokenizer decode sanity: FAIL ->\", repr(e))\n",
        "\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================\n",
        "# 4) generate_reply (간단/안정형)\n",
        "# ============================================================\n",
        "def generate_reply(\n",
        "    prompt: Union[str, List[str], Dict[str, Any]],\n",
        "    max_new_tokens: int = 96,\n",
        "    do_sample: bool = True,\n",
        "    temperature: float = 0.8,\n",
        "    top_p: float = 0.9,\n",
        "    repetition_penalty: float = 1.08,\n",
        "    debug: bool = False,\n",
        ") -> str:\n",
        "    # prompt -> str 정규화\n",
        "    if isinstance(prompt, dict):\n",
        "        prompt = prompt.get(\"prompt\") or prompt.get(\"text\") or prompt.get(\"review\") or str(prompt)\n",
        "    if isinstance(prompt, (list, tuple)):\n",
        "        prompt = \"\\n\".join(map(str, prompt))\n",
        "    prompt = str(prompt)\n",
        "\n",
        "    max_len = int(getattr(model.config, \"n_positions\", 1024) or 1024)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        padding=False,\n",
        "        add_special_tokens=True,\n",
        "    )\n",
        "\n",
        "    # 혹시라도 None이 끼면 여기서 즉시 알려줌\n",
        "    if any(x is None for x in inputs[\"input_ids\"].view(-1).tolist()):\n",
        "        raise ValueError(\"Tokenizer produced None inside input_ids. Tokenizer 상태가 비정상입니다.\")\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    gen_kwargs = dict(\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "    )\n",
        "    if do_sample:\n",
        "        gen_kwargs.update(dict(do_sample=True, temperature=temperature, top_p=top_p))\n",
        "    else:\n",
        "        gen_kwargs.update(dict(do_sample=False))\n",
        "\n",
        "    if debug:\n",
        "        print(\"prompt head:\", repr(prompt[:80]))\n",
        "        print(\"pad/eos:\", tokenizer.pad_token_id, tokenizer.eos_token_id)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "    full = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    # 프롬프트 제거\n",
        "    return full[len(prompt):].strip() if full.startswith(prompt) else full.strip()\n",
        "\n",
        "# ============================================================\n",
        "# Debugging block B: quick generation test\n",
        "# ============================================================\n",
        "print(\"Quick generation test:\")\n",
        "try:\n",
        "    print(generate_reply(\"리뷰: 배송이 늦어요\\n판매자 답변:\", max_new_tokens=60))\n",
        "except Exception as e:\n",
        "    print('this is error')\n",
        "    print(\"generate_reply failed ->\", repr(e))\n"
      ],
      "id": "7ff2c89fbd72024e"
    },
    {
      "metadata": {
        "id": "73f5a67bde84e9d8"
      },
      "cell_type": "markdown",
      "source": [
        "10. 프롬프트 3종 비교(단순/조건/강한 제약)"
      ],
      "id": "73f5a67bde84e9d8"
    },
    {
      "metadata": {
        "id": "b80027d34b35cc89",
        "ExecuteTime": {
          "end_time": "2025-12-31T05:37:01.956562Z",
          "start_time": "2025-12-31T05:37:01.707017Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962587f3-56ed-4f7a-8a76-dd196e9974a2"
      },
      "cell_type": "code",
      "source": [
        "sample_review_neg = \"배송이 너무 늦고 포장도 찢어져서 왔어요. 정말 실망입니다.\"\n",
        "sample_review_pos = \"배송도 빠르고 제품 품질이 좋아요. 재구매 의사 있습니다!\"\n",
        "\n",
        "def prompt_basic(review) -> str:\n",
        "    return f\"리뷰: {review}\\n판매자 답변:\"\n",
        "\n",
        "def prompt_with_rules(review, neg=True) -> str:\n",
        "    rules = (\n",
        "        \"규칙: 한국어 존댓말, 2~4문장.\\n\"\n",
        "        + (\"사과+해결책+보상(쿠폰/교환/환불 중 하나)을 반드시 포함.\\n\" if neg else \"감사+재구매 유도 포함.\\n\")\n",
        "    )\n",
        "    return f\"{rules}리뷰: {review}\\n판매자 답변:\"\n",
        "\n",
        "def prompt_project_style(review: str) -> str:\n",
        "    return (\n",
        "        \"### 역할: 당신은 온라인 쇼핑몰 판매자입니다.\\n\"\n",
        "        \"### 규칙:\\n\"\n",
        "        \"- 한국어 존댓말로 정중하게 작성\\n\"\n",
        "        \"- 2~4문장으로 간결하게\\n\"\n",
        "        \"- 부정 리뷰(1~2점)에는 사과+해결책+보상(쿠폰/교환/환불 중 하나) 포함\\n\"\n",
        "        \"- 긍정 리뷰(4~5점)에는 감사+재구매/재방문 유도 포함\\n\"\n",
        "        \"### 고객 리뷰:\\n\"\n",
        "        f\"{review}\\n\"\n",
        "        \"### 판매자 답변:\\n\"\n",
        "    )\n",
        "\n",
        "for p in [prompt_basic(sample_review_neg),\n",
        "          prompt_with_rules(sample_review_neg, neg=True),\n",
        "          prompt_project_style(sample_review_neg)]:\n",
        "    print(type(p), repr(p)[:120])\n",
        "    print(\"PROMPT:\\n\", p)\n",
        "    print(\"OUTPUT:\\n\", generate_reply(p))\n",
        "    print(\"=\"*80)\n",
        "\n",
        "for p in [prompt_basic(sample_review_pos),\n",
        "          prompt_with_rules(sample_review_pos, neg=False),\n",
        "          prompt_project_style(sample_review_pos)]:\n",
        "    print(\"PROMPT:\\n\", p)\n",
        "    print(\"OUTPUT:\\n\", generate_reply(p))\n",
        "    print(\"=\"*80)\n"
      ],
      "id": "b80027d34b35cc89",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'> '리뷰: 배송이 너무 늦고 포장도 찢어져서 왔어요. 정말 실망입니다.\\n판매자 답변:'\n",
            "PROMPT:\n",
            " 리뷰: 배송이 너무 늦고 포장도 찢어져서 왔어요. 정말 실망입니다.\n",
            "판매자 답변:\n",
            "OUTPUT:\n",
            " 안녕하세요, 고객님. 배송 관련하여 불편을 드려 진심으로 죄송합니다. 주문 정보와 문제 상황을 확인할 수 있도록 문의 남겨주시면 빠르게 도와드리겠습니다. 교환/환불 절차를 빠르게 진행하겠습니다.\n",
            "안녕하세요, 고객님. 배송 관련하여 불편하신 사항에 대해 확인해드리겠습니다. 소중한 후기 감사합니다. 앞으로도 더 좋은 상품과 서비스로 보답하겠습니다. 재구매해주시면 감사하겠습니다. 소중한 후기 감사합니다. 다음에도 만족하겠습니다. 재\n",
            "================================================================================\n",
            "<class 'str'> '규칙: 한국어 존댓말, 2~4문장.\\n사과+해결책+보상(쿠폰/교환/환불 중 하나)을 반드시 포함.\\n리뷰: 배송이 너무 늦고 포장도 찢어져서 왔어요. 정말 실망입니다.\\n판매자 답변:'\n",
            "PROMPT:\n",
            " 규칙: 한국어 존댓말, 2~4문장.\n",
            "사과+해결책+보상(쿠폰/교환/환불 중 하나)을 반드시 포함.\n",
            "리뷰: 배송이 너무 늦고 포장도 찢어져서 왔어요. 정말 실망입니다.\n",
            "판매자 답변:\n",
            "OUTPUT:\n",
            " 안녕하세요, 고객님. 주문 정보와 문제 상황을 확인할 수 있도록 문의 남겨주시면 빠르게 도와드리겠습니다. 교환/환불 관련하여 문의 남겨주시면 빠르게 도와드리겠습니다. 교환/환불 관련 사항 확인 후 쿠폰/부분환불 등 가능한 보상안을 안내드리겠습니다. 교환/환불 등 가능한 보상안을 안내드리겠습니다. 교환/환불 등 가능한 보상안을 안내드리겠습니다. 교환/환불 등의 가능한 보상안을 안내\n",
            "================================================================================\n",
            "<class 'str'> '### 역할: 당신은 온라인 쇼핑몰 판매자입니다.\\n### 규칙:\\n- 한국어 존댓말로 정중하게 작성\\n- 2~4문장으로 간결하게\\n- 부정 리뷰(1~2점)에는 사과+해결책+보상(쿠폰/교환/환불 중 하나) 포함\\n\n",
            "PROMPT:\n",
            " ### 역할: 당신은 온라인 쇼핑몰 판매자입니다.\n",
            "### 규칙:\n",
            "- 한국어 존댓말로 정중하게 작성\n",
            "- 2~4문장으로 간결하게\n",
            "- 부정 리뷰(1~2점)에는 사과+해결책+보상(쿠폰/교환/환불 중 하나) 포함\n",
            "- 긍정 리뷰(4~5점)에는 감사+재구매/재방문 유도 포함\n",
            "### 고객 리뷰:\n",
            "배송이 너무 늦고 포장도 찢어져서 왔어요. 정말 실망입니다.\n",
            "### 판매자 답변:\n",
            "\n",
            "OUTPUT:\n",
            " 안녕하세요, 고객님. 소중한 후기 감사합니다. 앞으로도 더 좋은 상품과 서비스로 보답하겠습니다. 다음에도 만족스러운 경험을 드리겠습니다. 다음에도 만족스러운 경험을 드리겠습니다. 재구매해주시면 감사하겠습니다. 다음에도 만족스러운 경험을 드리겠습니다. 다음에도 만족스런 경험을 드리겠습니다. 다음에도 만족스러운 경험을 드리겠습니다. 다음에도 만족스러운 경험을 드리겠습니다. 다음에도 만족스러운 경험을 드리겠습니다. 다음에도 만족스러운 경험을 드리겠습니다. 다음에도\n",
            "================================================================================\n",
            "PROMPT:\n",
            " 리뷰: 배송도 빠르고 제품 품질이 좋아요. 재구매 의사 있습니다!\n",
            "판매자 답변:\n",
            "OUTPUT:\n",
            " - 한국어 존댓말로 정중하게 작성\n",
            "- 2~4문장으로 간결하게\n",
            "- 부정 리뷰(4~5점)에는 사과+해결책+보상(쿠폰/교환/환불 중 하나) 포함\n",
            "- 긍정 리뷰(5~5점)에는 감사+재구매 의사 있음\n",
            "- 긍정 리뷰(6~5점)에는 감사+재구매 의사 있음\n",
            "- 긍정 리뷰(4~5점)에는 감사+재구매 의사 있음\n",
            "- 긍정 리뷰(4~5점)에는 감사+재\n",
            "================================================================================\n",
            "PROMPT:\n",
            " 규칙: 한국어 존댓말, 2~4문장.\n",
            "감사+재구매 유도 포함.\n",
            "리뷰: 배송도 빠르고 제품 품질이 좋아요. 재구매 의사 있습니다!\n",
            "판매자 답변:\n",
            "OUTPUT:\n",
            " 안녕하세요, 고객님. 배송 관련하여 만족하셨다니 기쁩니다.\n",
            "### 판매자 답변:\n",
            "안녕하세요, 고객님. 소중한 후기 감사합니다. 앞으로도 더 좋은 상품과 서비스로 보답하겠습니다. 소중한 후기 감사합니다. 다음에도 만족스러운 경험을 드리겠습니다.\n",
            "### 판매자 답변:\n",
            "안녕하세요, 고객님. 소중한 후기 감사합니다. 다음에도 만족스러운 경험을 드리겠습니다. 다음에도 만족스러운 경험을 드리\n",
            "================================================================================\n",
            "PROMPT:\n",
            " ### 역할: 당신은 온라인 쇼핑몰 판매자입니다.\n",
            "### 규칙:\n",
            "- 한국어 존댓말로 정중하게 작성\n",
            "- 2~4문장으로 간결하게\n",
            "- 부정 리뷰(1~2점)에는 사과+해결책+보상(쿠폰/교환/환불 중 하나) 포함\n",
            "- 긍정 리뷰(4~5점)에는 감사+재구매/재방문 유도 포함\n",
            "### 고객 리뷰:\n",
            "배송도 빠르고 제품 품질이 좋아요. 재구매 의사 있습니다!\n",
            "### 판매자 답변:\n",
            "\n",
            "OUTPUT:\n",
            " 안녕하세요, 고객님. 배송 관련하여 불편을 드려 진심으로 죄송합니다. 주문 정보와 문제 상황을 확인할 수 있도록 문의 남겨주시면 빠르게 도와드리겠습니다. 교환 또는 환불을 도와드리겠습니다. 교환 또는 환불을 도와드리겠습니다. 교환 또는 환불을 도와드리겠습니다. 교환 혹은 환불을 도와드리겠습니다. 교환 또는 환불을 도와드리겠습니다. 교환 또는 환불을 도와드리겠습니다. 교환 또는 환불을 도와드리겠습니다. 교환 및 환불 또는\n",
            "================================================================================\n"
          ]
        }
      ],
      "execution_count": 49
    },
    {
      "metadata": {
        "id": "21de0f2b77feaf54"
      },
      "cell_type": "markdown",
      "source": [
        "11. 규칙 준수 점수(간단 룰 기반)"
      ],
      "id": "21de0f2b77feaf54"
    },
    {
      "metadata": {
        "id": "796300a9b1260e83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "503a9b14-0701-4489-ecab-ceacf7f5233b"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg rule score: 1.44\n"
          ]
        }
      ],
      "execution_count": 50,
      "source": [
        "import re\n",
        "\n",
        "def score_reply(review_rating, reply):\n",
        "    score = 0\n",
        "    # 문장 수(대충 .?! 기준)\n",
        "    sentences = [s for s in re.split(r\"[.?!]\\s*\", reply) if s.strip()]\n",
        "    if 2 <= len(sentences) <= 4:\n",
        "        score += 1\n",
        "\n",
        "    if review_rating <= 2:\n",
        "        if any(k in reply for k in [\"죄송\", \"사과\"]):\n",
        "            score += 1\n",
        "        if any(k in reply for k in [\"교환\", \"환불\", \"쿠폰\", \"보상\"]):\n",
        "            score += 1\n",
        "    if review_rating >= 4:\n",
        "        if any(k in reply for k in [\"감사\", \"고맙\"]):\n",
        "            score += 1\n",
        "        if any(k in reply for k in [\"재구매\", \"다음에도\", \"또 이용\"]):\n",
        "            score += 1\n",
        "    return score\n",
        "\n",
        "# 샘플 50개로 점수 보기\n",
        "samples = random.sample(filtered, 50)\n",
        "total = 0\n",
        "for r, review in samples:\n",
        "    p = prompt_project_style(review)\n",
        "    out = generate_reply(p)\n",
        "    total += score_reply(r, out)\n",
        "\n",
        "print(\"avg rule score:\", total/50)\n"
      ],
      "id": "796300a9b1260e83"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "final_exam",
      "language": "python",
      "name": "final_exam"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      }
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9ee281eaebcf4167a8ef8b020ad57c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7eabb2708d6b43ab80fad23a882a155d",
              "IPY_MODEL_ad549afa0da14eff9b17a4b12fd57078",
              "IPY_MODEL_7f8cdfb62e974c77899f0b2f33478d57"
            ],
            "layout": "IPY_MODEL_d4247916794b49648448dbcea3892d3c"
          }
        },
        "7eabb2708d6b43ab80fad23a882a155d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_535348c4647543f4a9c26f156ca389a1",
            "placeholder": "​",
            "style": "IPY_MODEL_e17deef0ee544b7fbb6818071c3f8212",
            "value": "Map: 100%"
          }
        },
        "ad549afa0da14eff9b17a4b12fd57078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9ab9201543942ec9b4700908f1d9b54",
            "max": 19000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03abd0257a7749f6aa57d26476528e93",
            "value": 19000
          }
        },
        "7f8cdfb62e974c77899f0b2f33478d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d99101f26b114e38a087566f4a6868c9",
            "placeholder": "​",
            "style": "IPY_MODEL_461fab2872ca427dad527ee966526a7f",
            "value": " 19000/19000 [00:05&lt;00:00, 3801.35 examples/s]"
          }
        },
        "d4247916794b49648448dbcea3892d3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "535348c4647543f4a9c26f156ca389a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e17deef0ee544b7fbb6818071c3f8212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9ab9201543942ec9b4700908f1d9b54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03abd0257a7749f6aa57d26476528e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d99101f26b114e38a087566f4a6868c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "461fab2872ca427dad527ee966526a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db9590428ff94c27af62b512f23d94ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2be5fb1e06254dc7b2719a97cc5ab560",
              "IPY_MODEL_12ebce4469b84279ba96867125329f3f",
              "IPY_MODEL_99355e111b0049dc818ff880ba14276e"
            ],
            "layout": "IPY_MODEL_ffb57ae8f98c4da090b9a8d904ac1188"
          }
        },
        "2be5fb1e06254dc7b2719a97cc5ab560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6e8e8522761447b8cb7829e47e0e375",
            "placeholder": "​",
            "style": "IPY_MODEL_ce88bb889f66474d8a16dc8c13c582bf",
            "value": "Map: 100%"
          }
        },
        "12ebce4469b84279ba96867125329f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5e255c6683e4865a4c37672db7f2d37",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81ab9386dd05409d9f9459e8d015b428",
            "value": 1000
          }
        },
        "99355e111b0049dc818ff880ba14276e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a88ccef373034f3688298cdf8c56b03a",
            "placeholder": "​",
            "style": "IPY_MODEL_d4232ca1a1494ce096f252ebe6f53212",
            "value": " 1000/1000 [00:00&lt;00:00, 3418.72 examples/s]"
          }
        },
        "ffb57ae8f98c4da090b9a8d904ac1188": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6e8e8522761447b8cb7829e47e0e375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce88bb889f66474d8a16dc8c13c582bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5e255c6683e4865a4c37672db7f2d37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81ab9386dd05409d9f9459e8d015b428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a88ccef373034f3688298cdf8c56b03a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4232ca1a1494ce096f252ebe6f53212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}