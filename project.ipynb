{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 파이썬 버전 확인",
   "id": "ccd6c8e71c0953a7"
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "print(sys.version)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "zq011bQGe8ZH",
    "outputId": "efa484d7-8da1-4e1e-e7fd-5f42a4d53fef",
    "ExecuteTime": {
     "end_time": "2025-12-31T05:48:50.316563Z",
     "start_time": "2025-12-31T05:48:50.248568Z"
    }
   },
   "id": "zq011bQGe8ZH",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.14 (main, Oct 21 2025, 18:27:30) [Clang 20.1.8 ]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "id": "fa64466eca1790b6"
   },
   "cell_type": "markdown",
   "source": [
    "## 네이버 쇼핑 리뷰를 보고 판매자 답변 자동 생성하기(정중/사과/보상 제안)"
   ],
   "id": "fa64466eca1790b6"
  },
  {
   "metadata": {
    "id": "623f33a35a750969"
   },
   "cell_type": "markdown",
   "source": [
    "#### 데이터 출처 : https://github.com/bab2min/corpus"
   ],
   "id": "623f33a35a750969"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. 라이브러리 설치 (Colab GPU / Python 3.11)",
   "id": "49d892c43b5ef03f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T05:48:56.603556Z",
     "start_time": "2025-12-31T05:48:53.821756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Colab은 보통 torch 2.6.0+cu124가 이미 설치되어 있습니다.\n",
    "# 설치가 안 되어 있거나 버전을 맞추고 싶다면 주석 해제하여 실행하세요.\n",
    "# --- 맥북(Apple Silicon, CPU/MPS) ---\n",
    "!pip install -q -U pip\n",
    "!pip install -q \"torch>=2.2\" \"torchvision>=0.17\" \"torchaudio>=2.2\" \\\n",
    "    \"transformers>=4.41\" \"datasets>=2.19\" accelerate evaluate \\\n",
    "    \"scikit-learn>=1.2,<1.7\" tqdm peft\n",
    "\n",
    "# --- Colab GPU(T4 등, CUDA 12.4) ---\n",
    "# Colab에 기본 설치된 torch 2.6.0+cu124가 있으면 이 블록은 생략해도 됩니다.\n",
    "# 없거나 버전을 맞추고 싶을 때만 주석 해제해서 실행하세요.\n",
    "# !pip install -q --index-url https://download.pytorch.org/whl/cu124 \\\n",
    "#     torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124\n",
    "# 공통 패키지 설치\n",
    "!pip install -q -U \"transformers>=4.41\" \"datasets>=2.19\" accelerate evaluate \\\n",
    "    \"scikit-learn>=1.2,<1.7\" tqdm peft"
   ],
   "id": "8cddb851f8b3d441",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "id": "74d0ec5f2144ba54"
   },
   "cell_type": "markdown",
   "source": "2. 데이터 파싱",
   "id": "74d0ec5f2144ba54"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd4f60c06066e59",
    "outputId": "fba2161f-7cf4-462d-8a36-7c3a77b90c77",
    "ExecuteTime": {
     "end_time": "2025-12-31T05:49:03.970774Z",
     "start_time": "2025-12-31T05:49:03.752921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "RAW_PATH = Path(\"./naver_shopping.txt\")\n",
    "\n",
    "def load_reviews(min_len=5, max_n=None, seed=42):\n",
    "    random.seed(seed)\n",
    "    rows = []\n",
    "    with RAW_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(\"\\t\", 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            rating_s, text = parts\n",
    "            try:\n",
    "                rating = int(rating_s)\n",
    "            except:\n",
    "                continue\n",
    "            text = text.strip()\n",
    "            if len(text) < min_len:\n",
    "                continue\n",
    "            rows.append((rating, text))\n",
    "    random.shuffle(rows)\n",
    "    if max_n:\n",
    "        rows = rows[:max_n]\n",
    "    return rows\n",
    "\n",
    "rows = load_reviews(max_n=30000)  # 8GB면 처음엔 3만 이하 추천\n",
    "print(\"loaded:\", len(rows))\n",
    "print(rows[0])"
   ],
   "id": "dd4f60c06066e59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: 30000\n",
      "(2, '냄새랑 맛이 생각보다 너무 역해요')\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "id": "be7f27b09dd7efc"
   },
   "cell_type": "markdown",
   "source": "3. 불만 유형(키워드) 분류",
   "id": "be7f27b09dd7efc"
  },
  {
   "metadata": {
    "id": "dbc5da775caaccc2",
    "ExecuteTime": {
     "end_time": "2025-12-31T05:49:09.777103Z",
     "start_time": "2025-12-31T05:49:09.766866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "COMPLAINT_RULES = {\n",
    "    \"배송\": [\"배송\", \"늦\", \"지연\", \"도착\", \"택배\"],\n",
    "    \"품질/불량\": [\"불량\", \"하자\", \"고장\", \"깨\", \"찢\", \"누수\", \"작동\", \"불안정\"],\n",
    "    \"포장\": [\"포장\", \"박스\", \"파손\", \"훼손\", \"완충\"],\n",
    "    \"가격/가성비\": [\"비싸\", \"가격\", \"가성비\", \"싸\", \"할인\"],\n",
    "    \"응대/서비스\": [\"응대\", \"문의\", \"연락\", \"고객센터\", \"불친절\"],\n",
    "}\n",
    "\n",
    "def detect_topics(text: str):\n",
    "    topics = []\n",
    "    for topic, kws in COMPLAINT_RULES.items():\n",
    "        if any(kw in text for kw in kws):\n",
    "            topics.append(topic)\n",
    "    return topics[:2]  # 너무 많으면 1~2개만"
   ],
   "id": "dbc5da775caaccc2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "id": "b711a984fd0708ca"
   },
   "cell_type": "markdown",
   "source": "4. 템플릿 기반 판매자 답변 생성",
   "id": "b711a984fd0708ca"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "801fe53949cf5aa7",
    "outputId": "aac0c74b-0172-478d-ad5a-1e67f5f7fb39",
    "ExecuteTime": {
     "end_time": "2025-12-31T05:49:12.904240Z",
     "start_time": "2025-12-31T05:49:12.868637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_seller_reply(rating: int, review: str) -> str:\n",
    "    topics = detect_topics(review)\n",
    "    topic_phrase = \" / \".join(topics) if topics else None\n",
    "\n",
    "    # 보상 제안(너무 구체적 금액은 피하고 옵션만 제시)\n",
    "    compensation = random.choice([\n",
    "        \"교환 또는 환불을 도와드리겠습니다.\",\n",
    "        \"확인 후 쿠폰/부분환불 등 가능한 보상안을 안내드리겠습니다.\",\n",
    "        \"불편을 줄이기 위해 교환/환불 절차를 빠르게 진행해드리겠습니다.\"\n",
    "    ])\n",
    "\n",
    "    # 긍정(4~5)\n",
    "    if rating >= 4:\n",
    "        extra = \"앞으로도 더 좋은 상품과 서비스로 보답하겠습니다.\"\n",
    "        upsell = random.choice([\n",
    "            \"재구매해주시면 감사하겠습니다.\",\n",
    "            \"다음에도 만족스러운 경험을 드리겠습니다.\",\n",
    "            \"소중한 후기 감사합니다.\"\n",
    "        ])\n",
    "        if topic_phrase:\n",
    "            return f\"안녕하세요, 고객님. {topic_phrase} 관련하여 만족하셨다니 정말 기쁩니다. {extra} {upsell}\"\n",
    "        return f\"안녕하세요, 고객님. 소중한 후기 감사합니다. {extra} {upsell}\"\n",
    "\n",
    "    # 부정(1~2)\n",
    "    if rating <= 2:\n",
    "        apology = \"불편을 드려 진심으로 죄송합니다.\"\n",
    "        ask = \"주문 정보와 문제 상황을 확인할 수 있도록 문의 남겨주시면 빠르게 도와드리겠습니다.\"\n",
    "        if topic_phrase:\n",
    "            return f\"안녕하세요, 고객님. {topic_phrase} 관련하여 {apology} {ask} {compensation}\"\n",
    "        return f\"안녕하세요, 고객님. {apology} {ask} {compensation}\"\n",
    "\n",
    "    # 중립(3) — 과제에서는 제외해도 되고, 남겨도 됨\n",
    "    neutral = \"의견 남겨주셔서 감사합니다.\"\n",
    "    improve = \"말씀해주신 부분은 개선하여 더 나은 서비스로 보답하겠습니다.\"\n",
    "    if topic_phrase:\n",
    "        return f\"안녕하세요, 고객님. {topic_phrase} 관련하여 {neutral} {improve}\"\n",
    "    return f\"안녕하세요, 고객님. {neutral} {improve}\"\n",
    "\n",
    "# 샘플 확인\n",
    "for r, t in rows[:5]:\n",
    "    print(\"RATING:\", r)\n",
    "    print(\"REVIEW:\", t)\n",
    "    print(\"REPLY:\", make_seller_reply(r, t))\n",
    "    print(\"-\"*80)"
   ],
   "id": "801fe53949cf5aa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RATING: 2\n",
      "REVIEW: 냄새랑 맛이 생각보다 너무 역해요\n",
      "REPLY: 안녕하세요, 고객님. 불편을 드려 진심으로 죄송합니다. 주문 정보와 문제 상황을 확인할 수 있도록 문의 남겨주시면 빠르게 도와드리겠습니다. 확인 후 쿠폰/부분환불 등 가능한 보상안을 안내드리겠습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "RATING: 5\n",
      "REVIEW: 진짜대박좋아요만족^^\n",
      "REPLY: 안녕하세요, 고객님. 소중한 후기 감사합니다. 앞으로도 더 좋은 상품과 서비스로 보답하겠습니다. 다음에도 만족스러운 경험을 드리겠습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "RATING: 5\n",
      "REVIEW: 좋아요 배송도 빠르고 가겨도 마니 저렴하고~~^^\n",
      "REPLY: 안녕하세요, 고객님. 배송 관련하여 만족하셨다니 정말 기쁩니다. 앞으로도 더 좋은 상품과 서비스로 보답하겠습니다. 다음에도 만족스러운 경험을 드리겠습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "RATING: 1\n",
      "REVIEW: 1년 넘게 쓴 와이퍼보다 능력이 떨어지내요. 워셔액도 못 닦아내네요. 이런거 팔지 않으셨으면 좋겠네요\n",
      "REPLY: 안녕하세요, 고객님. 불편을 드려 진심으로 죄송합니다. 주문 정보와 문제 상황을 확인할 수 있도록 문의 남겨주시면 빠르게 도와드리겠습니다. 확인 후 쿠폰/부분환불 등 가능한 보상안을 안내드리겠습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "RATING: 2\n",
      "REVIEW: 진짜뚱뚱하신분만사셔야될듯\n",
      "REPLY: 안녕하세요, 고객님. 불편을 드려 진심으로 죄송합니다. 주문 정보와 문제 상황을 확인할 수 있도록 문의 남겨주시면 빠르게 도와드리겠습니다. 교환 또는 환불을 도와드리겠습니다.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "id": "519a046a30f10c57"
   },
   "cell_type": "markdown",
   "source": "5. 학습 데이터 구성 + split",
   "id": "519a046a30f10c57"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5e5a366160b74a4",
    "outputId": "42511e5c-ee11-44f0-a801-be702a1d0d52",
    "ExecuteTime": {
     "end_time": "2025-12-31T05:49:17.584362Z",
     "start_time": "2025-12-31T05:49:17.406684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_example(rating, review):\n",
    "    # 프롬프트: 역할/규칙/입력\n",
    "    prompt = (\n",
    "        \"### 역할: 당신은 온라인 쇼핑몰 판매자입니다.\\n\"\n",
    "        \"### 규칙:\\n\"\n",
    "        \"- 한국어 존댓말로 정중하게 작성\\n\"\n",
    "        \"- 2~4문장으로 간결하게\\n\"\n",
    "        \"- 부정 리뷰(1~2점)에는 사과+해결책+보상(쿠폰/교환/환불 중 하나) 포함\\n\"\n",
    "        \"- 긍정 리뷰(4~5점)에는 감사+재구매/재방문 유도 포함\\n\"\n",
    "        \"### 고객 리뷰:\\n\"\n",
    "        f\"{review}\\n\"\n",
    "        \"### 판매자 답변:\\n\"\n",
    "    )\n",
    "    completion = make_seller_reply(rating, review)\n",
    "    return {\"prompt\": prompt, \"completion\": completion, \"rating\": rating, \"review\": review}\n",
    "\n",
    "# 3점은 빼서 “명확한” 긍/부정만 학습(추천)\n",
    "filtered = [(r, t) for (r, t) in rows if r in (1,2,4,5)]\n",
    "data = [build_example(r, t) for r, t in filtered[:20000]]  # 최대 2만개만\n",
    "\n",
    "train_data, valid_data = train_test_split(data, test_size=0.05, random_state=42)\n",
    "\n",
    "print(len(train_data), len(valid_data))\n",
    "print(train_data[0][\"prompt\"])\n",
    "print(\"->\", train_data[0][\"completion\"])"
   ],
   "id": "a5e5a366160b74a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19000 1000\n",
      "### 역할: 당신은 온라인 쇼핑몰 판매자입니다.\n",
      "### 규칙:\n",
      "- 한국어 존댓말로 정중하게 작성\n",
      "- 2~4문장으로 간결하게\n",
      "- 부정 리뷰(1~2점)에는 사과+해결책+보상(쿠폰/교환/환불 중 하나) 포함\n",
      "- 긍정 리뷰(4~5점)에는 감사+재구매/재방문 유도 포함\n",
      "### 고객 리뷰:\n",
      "저렴하게 좋은 제품을 구매한거 같아 만족해요^^\n",
      "### 판매자 답변:\n",
      "\n",
      "-> 안녕하세요, 고객님. 소중한 후기 감사합니다. 앞으로도 더 좋은 상품과 서비스로 보답하겠습니다. 재구매해주시면 감사하겠습니다.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "id": "63d01d32d340f820"
   },
   "cell_type": "markdown",
   "source": "6. 모델/토크나이저 로드 + LoRA 적용",
   "id": "63d01d32d340f820"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c5eb6b96986443368c28221f5604e382",
      "fc385199a1814209aa03cb670d3b43c9",
      "ef1ed999f0e742d4a5be501deeea1eeb",
      "4257db7584164a0c969a9430ce44e26b",
      "2c71699d2c8445f386a61eddde2e5a8d",
      "0773618ae4c04403826a9328358eb953",
      "22740356561243149bd7baf103514790",
      "7746fe8c6e1b4318ad709298affa9012",
      "cce65fa004db40818a3a464ebc487cc7",
      "1a3f91a2444140f98e41c3c252da29bd",
      "65c6376157374bfbb4f8baa7d590c6ba",
      "6b17af190f5248b2a1ec0b119ba41fdb",
      "4fdcde7a9dd4423983260079fb4cec37",
      "f5f2adc7b38c4f268c7b072dea8b727a",
      "7c35cc2051fe46d69f642dd687a33a69",
      "ce9fa47042a04f0f941ffa7373c086b1",
      "7a23b70620a947f4a87f61aa0740a67c",
      "b50a21b2a6104224891b8c36217f7a76",
      "c1da9db8f054405ba6040e48f1ccf460",
      "9f77d9ed114e40609e4f4763d6610dff",
      "4f3838e8a5a54afeb98c3b7e1ceee4b5",
      "e3ad1ed8d805415bba0724ba4fc075d2"
     ]
    },
    "id": "37a82ec5c10af318",
    "outputId": "31ba6a87-2409-44f2-e888-c846443e131a",
    "ExecuteTime": {
     "end_time": "2025-12-31T05:49:38.804849Z",
     "start_time": "2025-12-31T05:49:30.366766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch, platform\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "MODEL_ID = \"skt/kogpt2-base-v2\"\n",
    "def pick_device():\n",
    "    # Colab GPU (CUDA)\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    # M1/M2 Apple Silicon (MPS)\n",
    "    if platform.system() == \"Darwin\" and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    # fallback CPU\n",
    "    return \"cpu\"\n",
    "device = pick_device()\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"mps available:\", torch.backends.mps.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu name:\", torch.cuda.get_device_name(0))\n",
    "print(\"device:\", device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "# GPT2류는 pad_token이 없는 경우가 많아서 eos로 맞추는 게 안전\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()  # 메모리 절약\n",
    "\n",
    "# LoRA 설정 (GPT-2 계열에서 흔히 쓰는 target)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 계열에 흔한 모듈명\n",
    "    fan_in_fan_out=True\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.to(device)"
   ],
   "id": "37a82ec5c10af318",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1\n",
      "cuda available: False\n",
      "mps available: True\n",
      "device: mps\n",
      "trainable params: 811,008 || all params: 125,975,040 || trainable%: 0.6438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(51200, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=3072)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "id": "8556d011fc204819"
   },
   "cell_type": "markdown",
   "source": "7. Dataset 만들기 + 토크나이징",
   "id": "8556d011fc204819"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "4b9e4431f1034e83aacf97adcd93b142",
      "3e0e17e210d944fb9ab151d540175931",
      "2d9621ab4fe5483f92b1acbedb899f65",
      "804a0ea8da94481a9ad8b64951ed47d8",
      "af3c6938af7841d68420448a6c729268",
      "5d78d97cf0074d37a15441db679a39e1",
      "62f3f9f4b432465982c5df617c1fa68d",
      "d683cd37c9f443359a0d01918e09f8a6",
      "22ed63e518344361aac2c27be08c39c7",
      "4553ba01a2384dd39125351eb713caa9",
      "71f1d303ea59483e8c46ed28b8b5c7ce",
      "966685494d654644a51881ec4d8cf8fd",
      "3350af8e708843f3a242ae9fcaeaacd9",
      "ff451db0e94a4b6dad645006b1c44b3f",
      "4b0b2792180144d186956c76334c8f1a",
      "af22bf996c7a475191a71607ab6fd3c8",
      "98871585fc7f4a819293afc91975ae53",
      "fdda0741aa614650a81c7e482f8d2394",
      "3f23f4da159545f09368841573a7a54f",
      "e5741e69f84540a4b9dda35c54a2fc6b",
      "d3ebb3ef64b6488daa09152fa70ddd18",
      "54a323672f1747bcbf89e129260971d4"
     ]
    },
    "id": "7ef416af57b56da8",
    "outputId": "4e5d974d-99af-4a69-e7b4-81fa90eae0fd",
    "ExecuteTime": {
     "end_time": "2025-12-31T05:49:43.899773Z",
     "start_time": "2025-12-31T05:49:41.763077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def pack_text(ex):\n",
    "    return ex[\"prompt\"] + ex[\"completion\"]\n",
    "\n",
    "train_ds = Dataset.from_dict({\"text\": [pack_text(x) for x in train_data]})\n",
    "valid_ds = Dataset.from_dict({\"text\": [pack_text(x) for x in valid_data]})\n",
    "\n",
    "MAX_LEN = 192  # 8GB 추천(256도 가능하지만 터질 수 있음)\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tok, batched=True, remove_columns=[\"text\"])\n",
    "valid_tok = valid_ds.map(tok, batched=True, remove_columns=[\"text\"])\n"
   ],
   "id": "7ef416af57b56da8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/19000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76836caae3ce46be89b706a91e9c4c84"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7c51fd9791044e4a93e6bc3735e1e1f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "id": "40afa70a49585bb9"
   },
   "cell_type": "markdown",
   "source": "8. Trainer로 학습",
   "id": "40afa70a49585bb9"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "60bf0f086be1ca5a",
    "outputId": "15ce5221-9ab7-4206-dce8-4f02e9264a2c",
    "ExecuteTime": {
     "end_time": "2025-12-31T05:50:21.622761Z",
     "start_time": "2025-12-31T05:49:48.545779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, random, inspect, platform\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# 디바이스 선택 (CUDA 우선, 그다음 MPS, 아니면 CPU)\n",
    "def pick_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if platform.system() == \"Darwin\" and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "device = pick_device()\n",
    "print(\"torch =\", torch.__version__)\n",
    "print(\"cuda available =\", torch.cuda.is_available())\n",
    "print(\"mps available =\", torch.backends.mps.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu name =\", torch.cuda.get_device_name(0))\n",
    "print(\"device =\", device)\n",
    "\n",
    "# 전제 체크\n",
    "assert \"tokenizer\" in globals(), \"tokenizer가 먼저 정의돼야 합니다.\"\n",
    "assert \"train_tok\" in globals(), \"train_tok(토큰화된 학습 데이터셋)이 먼저 정의돼야 합니다.\"\n",
    "valid_tok = globals().get(\"valid_tok\", None)\n",
    "MODEL_ID = globals().get(\"MODEL_ID\", \"skt/kogpt2-base-v2\")\n",
    "\n",
    "# tokenizer / special token 안전 확인\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"len(tokenizer) =\", len(tokenizer))\n",
    "print(\"pad_token_id =\", tokenizer.pad_token_id, \"eos_token_id =\", tokenizer.eos_token_id)\n",
    "\n",
    "# 베이스 모델 로드 + 임베딩 리사이즈\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "if tokenizer.eos_token_id is not None:\n",
    "    base_model.config.eos_token_id = tokenizer.eos_token_id\n",
    "base_model.config.use_cache = False\n",
    "base_model.gradient_checkpointing_enable()\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = globals().get(\"lora_config\", None)\n",
    "if lora_config is None:\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"c_attn\"],\n",
    "    )\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# 디바이스로 이동 (CUDA/MPS/CPU)\n",
    "model.to(device)\n",
    "try:\n",
    "    model.print_trainable_parameters()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 데이터 콜레이터\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# TrainingArguments\n",
    "OUTPUT_DIR = \"outputs/seller_reply_lora\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "common_kwargs = dict(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    max_steps=200,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=25,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    fp16=(device == \"cuda\"),  # CUDA에서만 fp16, MPS/CPU는 float32\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "try:\n",
    "    args = TrainingArguments(\n",
    "        **common_kwargs,\n",
    "        eval_strategy=\"steps\" if valid_tok is not None else \"no\",\n",
    "        eval_steps=200 if valid_tok is not None else None,\n",
    "    )\n",
    "except TypeError:\n",
    "    args = TrainingArguments(\n",
    "        **common_kwargs,\n",
    "        evaluation_strategy=\"steps\" if valid_tok is not None else \"no\",\n",
    "        eval_steps=200 if valid_tok is not None else None,\n",
    "    )\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=valid_tok if valid_tok is not None else None,\n",
    "    data_collator=collator,\n",
    ")\n",
    "if hasattr(trainer, \"label_names\"):\n",
    "    trainer.label_names = [\"labels\"]\n",
    "\n",
    "# 범위 체크\n",
    "emb_size = model.get_input_embeddings().weight.shape[0]\n",
    "idxs = random.sample(range(len(train_tok)), k=min(200, len(train_tok)))\n",
    "mx = max(max(train_tok[i][\"input_ids\"]) for i in idxs)\n",
    "print(\"model emb size =\", emb_size, \"| sample input_id max =\", mx)\n",
    "assert mx < emb_size, f\"토큰 id({mx})가 임베딩 크기({emb_size}) 이상입니다. resize가 적용됐는지 확인하세요.\"\n",
    "\n",
    "# 학습\n",
    "train_result = trainer.train()\n",
    "\n",
    "# 저장\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"✅ Done. Saved to:\", OUTPUT_DIR)\n",
    "print(\"Train result:\", train_result)"
   ],
   "id": "60bf0f086be1ca5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch = 2.9.1\n",
      "cuda available = False\n",
      "mps available = True\n",
      "device = mps\n",
      "len(tokenizer) = 51201\n",
      "pad_token_id = 51200 eos_token_id = 51200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,008 || all params: 125,975,808 || trainable%: 0.6438\n",
      "model emb size = 51201 | sample input_id max = 51200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/final_exam/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/200 00:20 < 13:27, 0.24 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "3f087609b69ead91a1f5c8f360029a1e"
     }
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 126\u001B[39m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m mx < emb_size, \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m토큰 id(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmx\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)가 임베딩 크기(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00memb_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) 이상입니다. resize가 적용됐는지 확인하세요.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    125\u001B[39m \u001B[38;5;66;03m# 학습\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m train_result = \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[38;5;66;03m# 저장\u001B[39;00m\n\u001B[32m    129\u001B[39m trainer.save_model(OUTPUT_DIR)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/final_exam/lib/python3.11/site-packages/transformers/trainer.py:2325\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2323\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2324\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2325\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2326\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2327\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2328\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2329\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/final_exam/lib/python3.11/site-packages/transformers/trainer.py:2674\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2667\u001B[39m context = (\n\u001B[32m   2668\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2669\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2670\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2671\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2672\u001B[39m )\n\u001B[32m   2673\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2674\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2676\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2677\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2678\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2679\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2680\u001B[39m ):\n\u001B[32m   2681\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2682\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/final_exam/lib/python3.11/site-packages/transformers/trainer.py:4071\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m   4068\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001B[32m   4069\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mscale_wrt_gas\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4071\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4073\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss.detach()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/final_exam/lib/python3.11/site-packages/accelerate/accelerator.py:2852\u001B[39m, in \u001B[36mAccelerator.backward\u001B[39m\u001B[34m(self, loss, **kwargs)\u001B[39m\n\u001B[32m   2850\u001B[39m     \u001B[38;5;28mself\u001B[39m.lomo_backward(loss, learning_rate)\n\u001B[32m   2851\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2852\u001B[39m     \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/final_exam/lib/python3.11/site-packages/torch/_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/final_exam/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/final_exam/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "id": "d2d37e368bf3342d"
   },
   "cell_type": "markdown",
   "source": "9. 생성 함수",
   "id": "d2d37e368bf3342d"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7ff2c89fbd72024e",
    "outputId": "1285e243-5e5e-4047-8c10-20e1733f114e"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipython-input-1-1008601399.py\", line 1, in <cell line: 0>\n",
      "    import torch\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 405, in <module>\n",
      "    from torch._C import *  # noqa: F403\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 216, in _lock_unlock_module\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
      "    traceback_info = getframeinfo(tb, context)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n",
      "    module = getmodule(object, filename)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/inspect.py\", line 994, in getmodule\n",
      "    f = getabsfile(module)\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/inspect.py\", line 963, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/inspect.py\", line 945, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 19, in exists\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m/tmp/ipython-input-1-1008601399.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtransformers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mAutoModelForCausalLM\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpeft\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mPeftModel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    404\u001B[0m         \u001B[0m_load_global_deps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 405\u001B[0;31m     \u001B[0;32mfrom\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_C\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m  \u001B[0;31m# noqa: F403\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    406\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_lock_unlock_module\u001B[0;34m(name)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2098\u001B[0m                         \u001B[0;31m# in the engines. This should return a list of strings.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2099\u001B[0;31m                         \u001B[0mstb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2100\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2099\u001B[0m                         \u001B[0mstb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2100\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2101\u001B[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001B[0m\u001B[1;32m   2102\u001B[0m                                             value, tb, tb_offset=tb_offset)\n\u001B[1;32m   2103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1365\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1366\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1367\u001B[0;31m         return FormattedTB.structured_traceback(\n\u001B[0m\u001B[1;32m   1368\u001B[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001B[1;32m   1369\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1265\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmode\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mverbose_modes\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1266\u001B[0m             \u001B[0;31m# Verbose modes need a full traceback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1267\u001B[0;31m             return VerboseTB.structured_traceback(\n\u001B[0m\u001B[1;32m   1268\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb_offset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumber_of_lines_of_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1269\u001B[0m             )\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1122\u001B[0m         \u001B[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1123\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1124\u001B[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001B[0m\u001B[1;32m   1125\u001B[0m                                                                tb_offset)\n\u001B[1;32m   1126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mformat_exception_as_a_whole\u001B[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[1;32m   1080\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1081\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1082\u001B[0;31m         \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfind_recursion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0morig_etype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1083\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1084\u001B[0m         \u001B[0mframes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat_records\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mfind_recursion\u001B[0;34m(etype, value, records)\u001B[0m\n\u001B[1;32m    380\u001B[0m     \u001B[0;31m# first frame (from in to out) that looks different.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    381\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_recursion_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 382\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    383\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    384\u001B[0m     \u001B[0;31m# Select filename, lineno, func_name to track frames with\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "import torch, platform, re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "MODEL_ID = \"skt/kogpt2-base-v2\"          # 셀 10에서 쓴 베이스 모델과 동일해야 함\n",
    "CKPT_DIR = \"outputs/seller_reply_lora\"   # 셀 10에서 저장한 경로와 동일\n",
    "\n",
    "def pick_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if platform.system() == \"Darwin\" and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "device = pick_device()\n",
    "print(\"device =\", device)\n",
    "\n",
    "# 1) ✅ tokenizer를 '저장 폴더'에서 로드 (len=51201 유지)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=False)\n",
    "\n",
    "def generate_reply(prompt: str,\n",
    "                   max_new_tokens: int = 96,\n",
    "                   do_sample: bool = True,\n",
    "                   temperature: float = 0.8,\n",
    "                   top_p: float = 0.9,\n",
    "                   repetition_penalty: float = 1.08) -> str:\n",
    "    \"\"\"프롬프트를 넣으면 '판매자 답변'만 생성해서 반환합니다.\n",
    "    (셀 9에서 로드한 tokenizer/model/device 사용)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 토크나이즈 → 디바이스로 이동\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "    )\n",
    "    if do_sample:\n",
    "        gen_kwargs.update(dict(do_sample=True, temperature=temperature, top_p=top_p))\n",
    "    else:\n",
    "        gen_kwargs.update(dict(do_sample=False))\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out_ids = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # 1) 디코딩 결과가 prompt로 시작하면 prompt 제거\n",
    "    if text.startswith(prompt):\n",
    "        reply = text[len(prompt):]\n",
    "    else:\n",
    "        # 2) 안전장치: '### 판매자 답변:' 이후만 가져오기\n",
    "        marker = \"### 판매자 답변:\"\n",
    "        reply = text.split(marker, 1)[-1] if marker in text else text\n",
    "\n",
    "    reply = reply.strip()\n",
    "\n",
    "    # 3) 모델이 다음 섹션(### ...)까지 계속 생성하면 거기서 끊기\n",
    "    reply = re.split(r\"\\n###\\s\", reply)[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "# 2) 베이스 모델 로드\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "\n",
    "# 3) ✅ 핵심: tokenizer 길이로 임베딩 리사이즈 (51200 -> 51201)\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# (안전) pad/eos 명시\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "if tokenizer.eos_token_id is not None:\n",
    "    base_model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 4) LoRA 어댑터 로드해서 합치기\n",
    "model = PeftModel.from_pretrained(base_model, CKPT_DIR)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"len(tokenizer) =\", len(tokenizer))\n",
    "print(\"emb size       =\", model.get_input_embeddings().weight.shape[0])"
   ],
   "id": "7ff2c89fbd72024e"
  },
  {
   "metadata": {
    "id": "73f5a67bde84e9d8"
   },
   "cell_type": "markdown",
   "source": "10. 프롬프트 3종 비교(단순/조건/강한 제약)",
   "id": "73f5a67bde84e9d8"
  },
  {
   "metadata": {
    "id": "b80027d34b35cc89",
    "ExecuteTime": {
     "end_time": "2025-12-31T05:37:01.956562Z",
     "start_time": "2025-12-31T05:37:01.707017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_review_neg = \"배송이 너무 늦고 포장도 찢어져서 왔어요. 정말 실망입니다.\"\n",
    "sample_review_pos = \"배송도 빠르고 제품 품질이 좋아요. 재구매 의사 있습니다!\"\n",
    "\n",
    "def prompt_basic(review):\n",
    "    return f\"리뷰: {review}\\n판매자 답변:\"\n",
    "\n",
    "def prompt_with_rules(review, neg=True):\n",
    "    rules = (\n",
    "        \"규칙: 한국어 존댓말, 2~4문장.\\n\"\n",
    "        + (\"사과+해결책+보상(쿠폰/교환/환불 중 하나)을 반드시 포함.\\n\" if neg else \"감사+재구매 유도 포함.\\n\")\n",
    "    )\n",
    "    return f\"{rules}리뷰: {review}\\n판매자 답변:\"\n",
    "\n",
    "def prompt_project_style(review):\n",
    "    # 학습 때 쓰던 포맷(일관성 최고)\n",
    "    return (\n",
    "        \"### 역할: 당신은 온라인 쇼핑몰 판매자입니다.\\n\"\n",
    "        \"### 규칙:\\n\"\n",
    "        \"- 한국어 존댓말로 정중하게 작성\\n\"\n",
    "        \"- 2~4문장으로 간결하게\\n\"\n",
    "        \"- 부정 리뷰에는 사과+해결책+보상 제안 포함\\n\"\n",
    "        \"- 긍정 리뷰에는 감사+재구매 유도 포함\\n\"\n",
    "        \"### 고객 리뷰:\\n\"\n",
    "        f\"{review}\\n\"\n",
    "        \"### 판매자 답변:\\n\"\n",
    "    )\n",
    "\n",
    "for p in [prompt_basic(sample_review_neg),\n",
    "          prompt_with_rules(sample_review_neg, neg=True),\n",
    "          prompt_project_style(sample_review_neg)]:\n",
    "    print(\"PROMPT:\\n\", p)\n",
    "    print(\"OUTPUT:\\n\", generate_reply(p))\n",
    "    print(\"=\"*80)\n",
    "\n",
    "for p in [prompt_basic(sample_review_pos),\n",
    "          prompt_with_rules(sample_review_pos, neg=False),\n",
    "          prompt_project_style(sample_review_pos)]:\n",
    "    print(\"PROMPT:\\n\", p)\n",
    "    print(\"OUTPUT:\\n\", generate_reply(p))\n",
    "    print(\"=\"*80)\n"
   ],
   "id": "b80027d34b35cc89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " 리뷰: 배송이 너무 늦고 포장도 찢어져서 왔어요. 정말 실망입니다.\n",
      "판매자 답변:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_reply' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 32\u001B[39m\n\u001B[32m     28\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m [prompt_basic(sample_review_neg),\n\u001B[32m     29\u001B[39m           prompt_with_rules(sample_review_neg, neg=\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[32m     30\u001B[39m           prompt_project_style(sample_review_neg)]:\n\u001B[32m     31\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mPROMPT:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m, p)\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mOUTPUT:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m, \u001B[43mgenerate_reply\u001B[49m(p))\n\u001B[32m     33\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m*\u001B[32m80\u001B[39m)\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m [prompt_basic(sample_review_pos),\n\u001B[32m     36\u001B[39m           prompt_with_rules(sample_review_pos, neg=\u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[32m     37\u001B[39m           prompt_project_style(sample_review_pos)]:\n",
      "\u001B[31mNameError\u001B[39m: name 'generate_reply' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "id": "21de0f2b77feaf54"
   },
   "cell_type": "markdown",
   "source": "11. 규칙 준수 점수(간단 룰 기반)",
   "id": "21de0f2b77feaf54"
  },
  {
   "metadata": {
    "id": "796300a9b1260e83"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "def score_reply(review_rating, reply):\n",
    "    score = 0\n",
    "    # 문장 수(대충 .?! 기준)\n",
    "    sentences = [s for s in re.split(r\"[.?!]\\s*\", reply) if s.strip()]\n",
    "    if 2 <= len(sentences) <= 4:\n",
    "        score += 1\n",
    "\n",
    "    if review_rating <= 2:\n",
    "        if any(k in reply for k in [\"죄송\", \"사과\"]):\n",
    "            score += 1\n",
    "        if any(k in reply for k in [\"교환\", \"환불\", \"쿠폰\", \"보상\"]):\n",
    "            score += 1\n",
    "    if review_rating >= 4:\n",
    "        if any(k in reply for k in [\"감사\", \"고맙\"]):\n",
    "            score += 1\n",
    "        if any(k in reply for k in [\"재구매\", \"다음에도\", \"또 이용\"]):\n",
    "            score += 1\n",
    "    return score\n",
    "\n",
    "# 샘플 50개로 점수 보기\n",
    "samples = random.sample(filtered, 50)\n",
    "total = 0\n",
    "for r, review in samples:\n",
    "    p = prompt_project_style(review)\n",
    "    out = generate_reply(p)\n",
    "    total += score_reply(r, out)\n",
    "\n",
    "print(\"avg rule score:\", total/50)\n"
   ],
   "id": "796300a9b1260e83"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_exam",
   "language": "python",
   "name": "final_exam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "runtime_attributes": {
    "runtime_version": "2025.07"
   }
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "c5eb6b96986443368c28221f5604e382": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc385199a1814209aa03cb670d3b43c9",
       "IPY_MODEL_ef1ed999f0e742d4a5be501deeea1eeb",
       "IPY_MODEL_4257db7584164a0c969a9430ce44e26b"
      ],
      "layout": "IPY_MODEL_2c71699d2c8445f386a61eddde2e5a8d"
     }
    },
    "fc385199a1814209aa03cb670d3b43c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0773618ae4c04403826a9328358eb953",
      "placeholder": "​",
      "style": "IPY_MODEL_22740356561243149bd7baf103514790",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "ef1ed999f0e742d4a5be501deeea1eeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7746fe8c6e1b4318ad709298affa9012",
      "max": 513302779,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cce65fa004db40818a3a464ebc487cc7",
      "value": 513302779
     }
    },
    "4257db7584164a0c969a9430ce44e26b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a3f91a2444140f98e41c3c252da29bd",
      "placeholder": "​",
      "style": "IPY_MODEL_65c6376157374bfbb4f8baa7d590c6ba",
      "value": " 513M/513M [00:05&lt;00:00, 113MB/s]"
     }
    },
    "2c71699d2c8445f386a61eddde2e5a8d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0773618ae4c04403826a9328358eb953": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22740356561243149bd7baf103514790": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7746fe8c6e1b4318ad709298affa9012": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cce65fa004db40818a3a464ebc487cc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1a3f91a2444140f98e41c3c252da29bd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65c6376157374bfbb4f8baa7d590c6ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b17af190f5248b2a1ec0b119ba41fdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4fdcde7a9dd4423983260079fb4cec37",
       "IPY_MODEL_f5f2adc7b38c4f268c7b072dea8b727a",
       "IPY_MODEL_7c35cc2051fe46d69f642dd687a33a69"
      ],
      "layout": "IPY_MODEL_ce9fa47042a04f0f941ffa7373c086b1"
     }
    },
    "4fdcde7a9dd4423983260079fb4cec37": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a23b70620a947f4a87f61aa0740a67c",
      "placeholder": "​",
      "style": "IPY_MODEL_b50a21b2a6104224891b8c36217f7a76",
      "value": "model.safetensors: 100%"
     }
    },
    "f5f2adc7b38c4f268c7b072dea8b727a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1da9db8f054405ba6040e48f1ccf460",
      "max": 513256494,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f77d9ed114e40609e4f4763d6610dff",
      "value": 513256494
     }
    },
    "7c35cc2051fe46d69f642dd687a33a69": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f3838e8a5a54afeb98c3b7e1ceee4b5",
      "placeholder": "​",
      "style": "IPY_MODEL_e3ad1ed8d805415bba0724ba4fc075d2",
      "value": " 513M/513M [00:10&lt;00:00, 46.9MB/s]"
     }
    },
    "ce9fa47042a04f0f941ffa7373c086b1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a23b70620a947f4a87f61aa0740a67c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b50a21b2a6104224891b8c36217f7a76": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1da9db8f054405ba6040e48f1ccf460": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f77d9ed114e40609e4f4763d6610dff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f3838e8a5a54afeb98c3b7e1ceee4b5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3ad1ed8d805415bba0724ba4fc075d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b9e4431f1034e83aacf97adcd93b142": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3e0e17e210d944fb9ab151d540175931",
       "IPY_MODEL_2d9621ab4fe5483f92b1acbedb899f65",
       "IPY_MODEL_804a0ea8da94481a9ad8b64951ed47d8"
      ],
      "layout": "IPY_MODEL_af3c6938af7841d68420448a6c729268"
     }
    },
    "3e0e17e210d944fb9ab151d540175931": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d78d97cf0074d37a15441db679a39e1",
      "placeholder": "​",
      "style": "IPY_MODEL_62f3f9f4b432465982c5df617c1fa68d",
      "value": "Map: 100%"
     }
    },
    "2d9621ab4fe5483f92b1acbedb899f65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d683cd37c9f443359a0d01918e09f8a6",
      "max": 19000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_22ed63e518344361aac2c27be08c39c7",
      "value": 19000
     }
    },
    "804a0ea8da94481a9ad8b64951ed47d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4553ba01a2384dd39125351eb713caa9",
      "placeholder": "​",
      "style": "IPY_MODEL_71f1d303ea59483e8c46ed28b8b5c7ce",
      "value": " 19000/19000 [00:06&lt;00:00, 3367.19 examples/s]"
     }
    },
    "af3c6938af7841d68420448a6c729268": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d78d97cf0074d37a15441db679a39e1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62f3f9f4b432465982c5df617c1fa68d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d683cd37c9f443359a0d01918e09f8a6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22ed63e518344361aac2c27be08c39c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4553ba01a2384dd39125351eb713caa9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71f1d303ea59483e8c46ed28b8b5c7ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "966685494d654644a51881ec4d8cf8fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3350af8e708843f3a242ae9fcaeaacd9",
       "IPY_MODEL_ff451db0e94a4b6dad645006b1c44b3f",
       "IPY_MODEL_4b0b2792180144d186956c76334c8f1a"
      ],
      "layout": "IPY_MODEL_af22bf996c7a475191a71607ab6fd3c8"
     }
    },
    "3350af8e708843f3a242ae9fcaeaacd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98871585fc7f4a819293afc91975ae53",
      "placeholder": "​",
      "style": "IPY_MODEL_fdda0741aa614650a81c7e482f8d2394",
      "value": "Map: 100%"
     }
    },
    "ff451db0e94a4b6dad645006b1c44b3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f23f4da159545f09368841573a7a54f",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e5741e69f84540a4b9dda35c54a2fc6b",
      "value": 1000
     }
    },
    "4b0b2792180144d186956c76334c8f1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3ebb3ef64b6488daa09152fa70ddd18",
      "placeholder": "​",
      "style": "IPY_MODEL_54a323672f1747bcbf89e129260971d4",
      "value": " 1000/1000 [00:00&lt;00:00, 3877.18 examples/s]"
     }
    },
    "af22bf996c7a475191a71607ab6fd3c8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98871585fc7f4a819293afc91975ae53": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdda0741aa614650a81c7e482f8d2394": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f23f4da159545f09368841573a7a54f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5741e69f84540a4b9dda35c54a2fc6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d3ebb3ef64b6488daa09152fa70ddd18": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54a323672f1747bcbf89e129260971d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
